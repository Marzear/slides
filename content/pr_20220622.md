---
title: 'Progress Report - 20220622'
# titie: 'Seminar - '
date: "2022-06-22"
theme: theme/mytheme.css
external_link: ""
---

# Progress Report - 20220622 <!-- .element: class="title" -->

<div class="title-name">
2022.06.22 <br>
Yu-Hung Wu @ IIS, Academia Sinica
</div>

---

## Outline

- Global Attention (GA) Investigation
- Stage 2 Implementation

---

## Global Attention (GA) Investigation

| Setting                               | Testing F1 Score | Testing EM Score |
| ------------------------------------- | ---------------- | ---------------- |
| **window size = 128, w/o GA**         | 58.17            | 53.20            |
| **avg. $\sigma$ = 10, w/o GA**        | 38.80            | 34.12            |
| **fixed $\sigma$ = 10, w/o GA**       | 34.46            | 30.23            |
| avg. $\sigma$ = 10, w/ GA (baseline)  | 69.38            | 64.38            |
| **avg. $\sigma$ = 100, w/o GA**       | 51.20            | 46.39            |
| **fixed $\sigma$ = 100, w/o GA**      | 39.74            | 35.62            |
| avg. $\sigma$ = 100, w/ GA (baseline) | 72.07            | 67.13            |

---

## Observations

1. Global Attention is a critical part in longformer (without GA, the performance dramatically decreases by 30 on both F1 and EM scores)

2. More $\sigma$ leads to better performance.

3. Letting the model to distribute $\sigma$ by itself leads to huge improvement.

---

## Stage 2

* For the weights of $U_{d}$ and $W_{p}$, I initialize the weights from a previous experiment (average $\sigma$ = 100, no global attention, no thresholding) and then freeze them.

* The maximum of the window size of each token is 256, and my goal is to distribute $\sigma$s (that equal to $w = 128$) to each token.

---

## Avg. $\sigma$ Calculation

* $U_{i, j} = e^{-\frac{(j-i)^{2}}{2\sigma}\}$, given a threshold $k$, we have $e^{-\frac{(j-i)^{2}}{2\sigma}\} > k$
* To get $\sigma$ by $k$ and $w$: $e^{-\frac{(j-i)^{2}}{2\sigma}\} = k$ 

    ➞  $-\frac{(j-i)^{2}}{2\sigma}\ = \log k$

    ➞  $\frac{(j-i)^{2}}{\sigma} = -2\log k$ 

    ➞  $\frac{w^{2}}{\sigma} = -2\log k$, where $w$ is the expected window size.

    ➞  $\sigma = \frac{w^{2}}{-2\log k}$

* For example, let $w = 128$, $k = 0.5$
  * $\frac{128^{2}}{\sigma} = -2\log 0.5$

    ➞ $\sigma$ = $\frac{128^{2}}{-2\log 0.5}$ 

    ➞ $\sigma = 11819$

---

## Stage 2 Results

* There are two settings of the experiment:
  1. threshold $k = 0.5$, $\sigma = 11819$ (calculate from the formula in the previous slide)
  2. threshold $k = 0.6$, $\sigma = 16036$


<div id="left"> 

![](attachments/2022-06-20-15-28-48.png) <!-- .element: class="img100" -->

</div>
<div id="right">

![](attachments/2022-06-20-15-30-59.png) <!-- .element: class="img100" -->

</div>

---

## Stage 2 Results.

* ALL reported performances do NOT include global attention.

1. Fixed window (baseline)

| Window size | Testing F1 Score | Testing EM Score |
| ----------- | ---------------- | ---------------- |
| 128         | 58.17            | 53.20            |
| 108         | 58.00            | 53.19            |

2. Fixed $\sigma$ (baseline) (w = 128)

| $\sigma$ | Testing F1 Score | Testing EM Score |
| -------- | ---------------- | ---------------- |
| 11819    | 59.04            | 54.17            |

3. Constrainted $\sigma$ (max w = 256, avg w = 128)

| Threshold | Avg. $\sigma$ | ACTUAL avg. window size | Testing F1 Score | Testing EM Score |
| --------- | ------------- | ----------------------- | ---------------- | ---------------- |
| 0.5       | 11819         | **108**                 | 59.68            | 54.87            |
| 0.6       | 16036         | 待補                    | 待補             | 待補             |

---

## Observations

1. Performance increased as window size incresed.

2. Same as the experiments of adding global attention, multiplying the bell curve to attention score boosted the performance.

3. In the third experiment, the actual window size is usually smaller than the origin window size (128), however, the performance even increased.

4. 待補 The more the threshold (and the more the $\sigma$), the more the performance.

---

## Todo

1. Explore a way to add global attention to stage 2.

2. 