---
title: 'seminar - 20211224'
date: "2021-12-24"
theme: theme/mytheme.css
external_link: ""
---

# PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable <!-- .element: class="title" -->
## ACL 2020 <!-- .element: class="subtitle" -->

<div class="title-name">
2021.12.24 <br>
Yu-Hung Wu @ Academia Sinica
</div>

https://arxiv.org/abs/1910.07931 <!-- .element: class="footnote" -->

---

## Outline

- Introduction
- PLATO
- Experiments
- Conclusion

---

## Introduction  <!-- .element: class="section-title" -->

----

## Background

- Dialogue generation in open-domain conversations is a challenging task, due to
    1. limited corpus of human conversations.
    2. complex background knowledge.
    3. Diverse relationships between utterances.

- Pre-trained large-scale language models can be used for generation after being fine-tuned.

- However, there are some deficiencies in performance while directly fine-tuning BERT on small conversation datasets.

----

## Problems

1. The underlying linguistic patterns in human conversations can be highly different from those in general text.

2. The training mode of uni-directional dialogue generation is also distinct from that of bi-directional natural language understating as applied in BERT.

3. There exists a ***one-to-many*** relationship in dialogue generation.

----

## Solutions

1. The underlying linguistic patterns in human conversations can be highly different from those in general text.
    - Large-scale Reddit and Twitter conversations are utilized.
2. The training mode of uni-directional dialogue generation is also distinct from that of bi-directional natural language understating as applied in BERT.
    - A flexible paradigm integrating uni- and bi-directional processing is employed.
3. There exists a ***one-to-many*** relationship in dialogue generation.
    - Discrete latent variable is introduced.

----

## Latent Variable

- The latent variable gets exempted from the restriction of human annotations and can be learned automatically from the corpus.

- There are 2 tasks in this work:
    1. Response generation: Based on the context and latent variable, the generation task tries to maximize the likelihood of the target response.

    2. Latent act recognition: Based on the given context and target response, the task aims to estimate the latent variable.

---

## PLATO <!-- .element: class="section-title" -->

----

## Basic Idea

- The main purpose is to encode discrete latent variables into transformer blocks for one-to-many relationship modeling.

- In the model, there are 3 elements:
    1. dialogue context (dialogue history, knowledge...) $c$
    2. target response $r$
    3. latent variable $z$: a $K$-way categorical variable $z ∈ [1, K]$

----

## Probabilistic Relationships

- Response generation (gray lines) - Given a context $c$ and latent variable $z$, conditioned on the context and one selected latent speech act, the response $r$ is generated as $p(r|c, z)$.

- Latent act recognition (dashed blue lines): Given a pair of context $c$ and response $r$, the underlying latent speech act $z$ can be estimated as $p(z|c, r)$.

![](attachments/2021-12-22-17-54-36.png) <!-- .element: class="img95" -->

----

## Model Architecture - 1 

![](attachments/2021-12-22-18-00-58.png) <!-- .element: class="img100" -->

- The latent act recognition shares network parameters with response generation.
- The response generation is a uni-directional decoding process, each token in the response only attends to those before it.

----

## Model Architecture - 2 

![](attachments/2021-12-22-18-00-58.png) <!-- .element: class="img100" -->

- The latent act recognition works with a special mask symbol $[M]$ as input, it keeps collecting information from the context and target response.

----

## Input Representation

![](attachments/2021-12-22-18-14-34.png) <!-- .element: class="img100" -->

- Given that $z$ is one $K$-way categorical variable, its token embedding $E[z]$ is mapped from the latent embedding space $ E_{z} ∈ \mathbb{R}^{K×D} $.

- Role embeddings are employed to differentiate the characters evolved in the conversation.

----

## Latent Embeddings
```python
'''
post_embed.size = [batch_sz, BERT dimension, seq length]
post_logits.size = [batch_sz, K]
latent_embeddings.size = [batch_sz, K, BERT dimension]
'''
post_embed = bert_embed[:, 0]
post_logits = self.post_network(post_embed) #FC
if is_training:
    z = F.gumbel_softmax(post_logits, self.tau)
else:
    indices = layers.argmax(post_logits, axis=1)
    z = layers.one_hot(F.unsqueeze(indices, [1]), self.num_latent)
latent_embed = layers.matmul(z, latent_embeddings)
```

* ```post_embed``` is the final hidden state of token $[M]$ (in the latent act recognition task).
* ```post_network``` is a fully connected layer, mapping the embedding from BERT dimension to $K$.
* ```latent_embeddings``` is a $[K, 768]$ matrix, mapping $z$ to the latent variable token embedding $E[z]$.


----

## Pre-training Objectives

- In this task, there are 3 losses:
    1. negative log-likelihood (NLL) 
    2. bag-of-words (BOW)
    3. response selection (RS) loss

- The total objective of our pre-training model is to minimize the integrated loss:
    ![](attachments/2021-12-22-18-28-15.png)

----

## Negative Log-likelihood (NLL) loss 

![](attachments/2021-12-22-18-30-11.png) <!-- .element: class="img60" -->

- $z$ is the latent speech act of this training pair $(c, r)$, sampled from the probability distribution $p(z|c, r)$.

- $p(z|c,r)$ is estimated through the task of latent act recognition:
    ![](attachments/2021-12-22-18-32-48.png)
    where $h[M] ∈ \mathbb{R}^{D}$ is the final hidden state of the special mask, $W_{1} ∈ \mathbb{R}^{K×D}$ and $b_{1} ∈ \mathbb{R}^{K}$ denote
the weight matrices of one fully-connected layer.

----

## BOW (bag-of-words) loss

![](attachments/2021-12-23-17-09-12.png)

- Aims to facilitate the training process of latent discrete variables.

- The function $f$ tries to predict the words within the target response in a non-autoregressive way:

![](attachments/2021-12-23-17-11-31.png)

where $h_{z}$ is the final hidden state of the latent variable and $|V|$ is the vocabulary size. $f_{r_{t}}$ denotes the estimated probability of word $r_{t}$.

----

## NLL loss vs. BOW loss

- NLL loss is one of a useful metrics in response generation.

- BOW loss discards the order of words and forces the latent variable to capture the global information of the target response.

----

## RS (response generation) loss - 1

- Response selection helps distinguish whether the response is relevant with the dialogue context and consistent with the background knowledge.

-  its score can be regarded as an indicator of coherence during inference, helping to select the most coherent one from multiple candidate responses.

----

## RS (response generation) loss - 2

- The positive training samples come from the dialogue context and corresponding target response $(c, r)$, with label $l_{r} = 1$.

- The negative samples are created by randomly selecting responses from the corpus $(c, r^{−})$, with label $l_{r^{−}} = 0$.

- The binary cross-entropy loss of response selection is defined as follows:![](attachments/2021-12-23-17-31-35.png)

----

## Pre-training Summary

1. Latent Act Recognition

    - Given a pair of context and target response, estimate the posterior distribution $p(z|c, r)$.
    - Randomly select $r^{-}$ and calculate $L_{RS}$.

2. Response Generation
    - With the sampled latent value $z ∼ p(z|c, r)$, calculate $L_{NLL}$ and $L_{BOW}$.

3. Optimization
    - Sum up to obtain $L$, and update network parameters with back-propagation.

----

## Hyper-parameters Used in Pre-training

- Max length of context = 256, response = 50

- Transformer blocks = 12, dimension = 768

- batch size = 64, K (number of latents) = 20

- 8 Nvidia Tesla V100 32G GPU cards for 3.5M steps, taking about two weeks to reach convergence.

----

## Fine-tuning

- The fine-tuning on small conversation datasets can be carried out by following the training objectives of the pre-training procedure.

- During inference, we can
    1. Conditioned on each latent value $z ∈ [1, K]$, generate corresponding candidate response $r$.
    2. Calculate the probability for each response $p(l_{r} = 1|c, r)$ and select the one with highest coherence value as the final response.

---

## Experiments <!-- .element: class="section-title" -->

----

## Datasets

- Persona-Chat: a knowledge grounded conversation dataset.

- Daily Dialog: a chit-chat dataset, which contains high-quality human conversations about daily life.

- DSTC7-AVSD: a conversational question answering dataset.
    -  the system need to generate an answer given dialogue context and background knowledge.

![](attachments/2021-12-23-18-04-29.png) <!-- .element: class="img100" -->

----

## Baseline

- Persona-Chat and Daily Dialog: Sequence to sequence with attention

- DSTC7-AVSD: DSTC7 baseline system

----

## Evaluation Metrics

- BLEU score: measures the n-gram overlap between generated response and the target response.

- Distinct-1/2: measures the generation diversity, which is defined as the number of distinct uni- or bi-grams divided by the total amount of generated words

- Knowledge Recall/Precision/F1: measures the degree of informativeness.

----

## Experiments - 1

![](attachments/2021-12-23-18-07-48.png) <!-- .element: class="img100" -->

----

## Experiments - 2

![](attachments/2021-12-23-18-14-37.png) <!-- .element: class="img100" -->

----

## Sample responses

![](attachments/2021-12-23-18-32-25.png) <!-- .element: class="img100" -->

----

## Ablation Studies

![](attachments/2021-12-23-18-41-16.png) <!-- .element: class="img100" -->

* Model 1.2 vs. 1.3: Reflects that bi-directional attention mask should apply to context information.

* Group 2 vs. 1: Reflects Twitter and Reddit conversations do improve the generated text.

* Model 2.2 vs. 3.1: Reflects that discrete latent variable is able to boost the quality of response generation.

---

## Conclusion <!-- .element: class="section-title" -->

----

## Conclusion

- Incorporated with latent discrete variables, PLATO has the ability to deal with one-to-many relationship modeling.

- Two reciprocal tasks of response generation and latent recognition are carried out simultaneously on large-scale conversation datasets.

- The results demonstrate that our model obtains significant improvements over the other state-of-the-art methods.

- The model can be potentially improved with more fine-grained latent variables.