---
title: 'Seminar - 20220610'
date: "2022-06-10"
theme: theme/mytheme.css
external_link: ""
---

# ProphetChat: Enhancing Dialogue Generation with Simulation of Future Conversation <!-- .element: class="title" -->
## Liu et al., ACL 2022<!-- .element: class="subtitle" -->

<div class="title-name">
2022.06.10 <br>
Yu-Hung Wu @ IIS, Academia Sinica
</div>

https://aclanthology.org/2022.acl-long.68/ <!-- .element: class="footnote" -->

---

## Outline

- Previous Model
- Proposed Model
- Results & Studies
- Conclusion

---

## Previous Model  <!-- .element: class="section-title" -->

----

## Dialogue Generation in Open-domain Conversations

- Recent dialogue systems usually utilize the dialogue history to generate the response, i.e. estimate the probability of $P(response|history)$.

- There are some common problems in such systems:

    1. Generating bland responses (e.g., yeah, ha ha) is the most critical problem.

    2. It also poses a greater **one-to-many** problem than is typical in other text generation tasks.

    3. The underlying linguistic patterns in human conversations can be highly different from those in general text.

----

## DialoGPT (Zhang et al., 2020)

- DialoGPT is formulated as an autoregressive language model, and uses the multi-layer transformer as model architecture.

- The model is trained on large-scale dialogue pairs/sessions extracted from Reddit discussion chains.

- DialoGPT successfully captures the joint distribution of $P(target|source)$ in conversational flow, which achieves SOTA results in 2020.

----

## DialoGPT (Zhang et al., 2020).

- First, concatenate all dialog turns within a dialogue session into a long text $S = x_{1}, x_{2}, ... , x_{m}$ ($m$ is the sequence length), ended by the ```<end-of-text>``` token.

- The conditional probability of $P(T|S)$ can be written as the product of a series of conditional probabilities:
    ![](attachments/2022-06-08-21-46-12.png)

    where $m$ is the length of the history sequence.

----

## Maximum Mutual Information (MMI)

- MMI is a solution the problem that the response was sometimes too bland or uninformative.

- Given a dialogue in the training set:
  - A: How old are you?
  - B: I'm 22 years old.
  - A: Oh, that's terrible.

- To train the target model, we use the following as input sequence:
  - ```[CLS]How old are you?[SEP]I'm 22 years old.[SEP]Oh, that's terrible.[SEP]```

----

## Maximum Mutual Information (MMI).

- Now, we train an additional "MMI model". MMI is the backward version of the original model, it's goal is to *predict source sentences from given responses.*.
    - Original training objective: maximize $P(Target|Source)$, MMI training objective: $P(Source|Target)$.

- Thus, the input is the **reversed** sentence of the original model:
  - ```[CLS]Oh, that's terrible.[SEP]I'm 22 years old.[SEP]How old are you?[SEP]```

----

## How to Use the MMI Model?

1. Use the target model to generate a set of responses (use different sampling method to generate diverse responses, such as top-k), we denote it as *S*.
    - $S = [r_{1}, r_{2}, ... ,r_{n}]$, where $n$ is the number of candidate responses.

2. for each response in $S$, use the MMI model to calculate it's perplexity score. The input of the MMI model is backwarded.

3. Choose the one with least perplexity, this is the final response.

---

## Proposed Model  <!-- .element: class="section-title" -->

----

## Motivation

- Since there exists a ***one-to-many*** relationship in dialogue generation, generating a desired response solely based on the historical information is not easy.

- If the chatbot can foresee in advance what the user would talk about after receiving its response, it could possibly provide a more informative response.

- Previous models usually consider $P(response|history)$, the proposed model considers $P(response|history, future)$.

----

## Motivation.

- Some PLMs such as BERT use bi-directional information instead of just considering the tokens on its left side, and have bring significant improvement.

- The model thus uses the "right side information" (i.e. dialogue future) to enhance the generated response.

----

## Example

![](attachments/2022-06-08-21-47-26.png) <!-- .element: class="img65" -->

----

## Overall Framework
![](attachments/2022-06-08-21-48-36.png) <!-- .element: class="img110" -->

- Generate responses → select $k$ → generate futures → select $k$ → generate more responses → select 1

----

## Generating First-pass Responses

![](attachments/2022-06-08-21-48-26.png) <!-- .element: class="img80" -->

- Given a dialogue history $h$, first use $G_{F}$ (a forward generator, you can just regard as a normal GPT-2) to generate $n$ responses using top-$k$ sampling.

- The selector $S$ then calculates the quality scores for all history-response pairs.

- Keep the $k$-best responses at hand while discarding the others.

----

## Generating Futures

![](attachments/2022-06-08-21-49-57.png) <!-- .element: class="img80" -->

- Now, we have $k$ first-pass responses. We use $G_{F}$ (**exactly the same as the previous $G_{F}$**) to generate $n$ futures for each response. Thus, we obtain $k · n$ history-response-future dialogue triplets.

- We again use to the selector $S$ to calculate the quality scores of all the generated futures.

----

## Generating Futures.

![](attachments/2022-06-08-21-49-57.png) <!-- .element: class="img80" -->

- Considering that the responses are not equal in quality, we additionally multiply each score of future with the score of its ancestral response to get the final ranking score.

- Again, we keep the $k$-best futures at hand while discarding the others.

----

## Ensemble Generation

![](attachments/2022-06-09-00-38-58.png) <!-- .element: class="img80" -->

- The goal of ensemble generation is to generate to second-pass responses (i.e. the responses that has consider the simulated futures).

- The responses are generate base on both *history-conditioned* $G_{F}$ (yes, the same $G_{F}$ again) and *future-conditioned* $G_{B}$ (MMI model).

----

## Ensemble Generation.

- The response $r$ using the per-step weighted ensemble of $G_{F}$ and $G_{B}$ conditioned on $h$ and $f$:

  ![](attachments/2022-06-09-00-46-17.png) <!-- .element: class="img70" -->

- $w$ is a ***learnable*** weight:
  - Using a ***trainable*** gate $g$ which takes the last hidden states from $G_{F}$ and $G_{B}$ as inputs and calculates an ensemble weighting score w using an MLP with sigmoid activation.

- We use the above method to generate $n$ responses for each future, and thus generate $k · n$ responses.

----

## Final Response Generation

![](attachments/2022-06-09-01-08-13.png) <!-- .element: class="img80" -->

- To make full use of the $k$-best first-pass responses, we finally re-rank the $k + k · n$ responses with $S$ and consider the top-ranked response as our system outputs.

----

## Framework Summary

![](attachments/2022-06-08-21-48-36.png) <!-- .element: class="img70" -->

- There are 4 components in the framework:
  1. $G_{F}$: the forward model.
  2. $G_{B}$: the backward model.
  3. $g$: the gate used to compute the weight between $G_{F}$ and $G_{B}$.
  4. $S$: the selector to evaluate the utterances.

- Generate responses → select $k$ → generate futures → select $k$ → generate more responses → select 1