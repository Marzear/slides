<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>reveal-md</title>
    <link rel="shortcut icon" href="./favicon.ico" />
    <link rel="stylesheet" href="./dist/reset.css" />
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="/_assets/theme/mytheme.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />


  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">

# DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding <!-- .element: class="title" -->
## SIGIR 2020<!-- .element: class="subtitle" -->

<div class="title-name">
2021.10.15 <br>
Yu-Hung Wu @ Academia Sinica
</div>

https://arxiv.org/abs/2002.12591 <!-- .element: class="footnote" -->
</script></section><section  data-markdown><script type="text/template">
## Outline

- Background
- Proposed Model
- Experiments
- Conclusion
</script></section><section ><section data-markdown><script type="text/template">
## Background <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Open-domain Question Answering

- This task requires a machine to find the answer by referring to large unstructured text corpora without knowing which documents may contain the answer.

- Previous works usually use RNN and CNN based approaches.

- With high-quality contextual encodings, BERT-based approaches have dominated the leaderboards and significantly outperformed previous approaches.
</script></section><section data-markdown><script type="text/template">
## Retrieve and Read

- Combines information retrieval (IR) and machine reading comprehension (MRC) modules.

- The former retrieves the documents using off-the-shelf IR systems based on TF-IDF or BM25, and the latter reads the retrieved documents to extract answer.

P.S. BM25 formula:

![](attachments/2021-10-14-23-11-22.png) <!-- .element: class="img95" -->
</script></section><section data-markdown><script type="text/template">
## Retrieve and Read

![](attachments/2021-10-11-21-42-40.png) <!-- .element: class="img95" -->
</script></section><section data-markdown><script type="text/template">
## Problems

- Classic model such as TF-IDF and Best Match Models have shallow understanding of the context. Thus, documents that contain the correct answer may not be ranked among the top by IR systems.

- We can't simply feed more documents into the reader module to increase the chance of hitting under-ranked documents that contain the answer, it can be computationally expensive.
</script></section><section data-markdown><script type="text/template">
## BERT-based Reranker for Retrieval

- A binary classifier to filter retrieved documents before feeding them into the reader module. 

![](attachments/2021-10-11-22-00-35.png) <!-- .element: class="img85" -->
</script></section><section data-markdown><script type="text/template">
## Problems

- Need to score all documents for a single query. 
  - Testing set: 80 queries * 100,000 docs = 8,000,000 computations

- This severe efficiency problem prohibits existing approaches for open-domain QA from being deployed as real-time QA systems.

- Such method can't outperform classic models
</script></section><section data-markdown><script type="text/template">
## My Previous Work

- Use BERT to rerank BM25 scores.

- Just rerank the top 1000 documents retrieved from BM25 model.

- Rescore the relavance score with

    $Score = 1 \times Score_{BM25} + Weight \times Score_{BERT}$
</script></section><section data-markdown><script type="text/template">
## My Previous Work - Results

| Method           | MAP@1000   |
| ---------------- | ---------- |
| BM25 + BERT      | **45.216** |
| BM25 only        | 39.136     |
| BERT only        | 30.248     |
| Random Documents | 0.007      |
</script></section><section data-markdown><script type="text/template">
## Mini Summary

- State-of-the-art approaches typically follow the “retrieve and read” pipeline

- Using classic models as the retrieval module is not enough, since such models have shallow understanding of the context

- However, use BERT to rerank the BM25 retreival can't even outperform the original BM25, and also time consuming

- We need a better solution
</script></section></section><section ><section data-markdown><script type="text/template">
## Proposed Model <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Basic Idea

- Two separate BERT models
  1. An online BERT which encodes the question only once
  2. An offline BERT which pre-encodes all the documents and caches their encodings

- The decoupled encodings of question and document are then fed into the Transformer layers with global position and type embeddings for question-document interactions.
</script></section><section data-markdown><script type="text/template">
## DC-BERT

![](attachments/2021-10-12-01-04-52.png) <!-- .element: class="img85" -->
</script></section><section data-markdown><script type="text/template">
## Dual-BERT component - Idea

- During training, the parameters of both BERT models are updated to optimize the learning objective.

- Once the model is trained, we pre-encode all the documents and store their encodings in an offline cache.

- During testing, we encode the question only once using the online BERT, and instantly read out the cached encodings of all the candidate documents.
</script></section><section data-markdown><script type="text/template">
## Dual-BERT component - Computation Complexity

- Naïve BERT method: *O(N<sub>q</sub>N<sub>d</sub>(L<sub>q</sub> + L<sub>d</sub>)<sup>2</sup>)*

- DC-BERT: *O(N<sub>q</sub>(L<sup>2</sup><sub>q</sub> + N<sub>d</sub>L<sup>2</sup><sub>d</sub>))*

- DC-BERT(with cache): *O(N<sub>q</sub>L<sup>2</sup><sub>q</sub>)*

Where N<sub>q</sub> denote the number of questions, N<sub>d</sub> denote the number of retrieved documents for each question, L<sub>q</sub> and L<sub>d</sub> denote the average number of tokens of each question and document, respectively.
</script></section><section data-markdown><script type="text/template">
## Transformer component

![](attachments/2021-10-12-12-07-17.png) <!-- .element: class="img90" -->
</script></section><section data-markdown><script type="text/template">
## Transformer component

- We obtain the question encoding T ∈ R<sup>N×d</sup> and the document encoding T′ ∈ R<sup>M×d</sup>. In this paper, we just concat the two embeddings directly.

- For the embedding layers, we have
  - Global position embeddings E<sub>Pi</sub> ∈ R<sup>d</sup>: Re-encode the token at position Pi in the concatenated question-document encoding sequence.
  - Segmentation Embeddings: To differentiate whether the encoded token is from question or document.
</script></section><section data-markdown><script type="text/template">
## Transformer component - Initialize

- Previous studies observed that the lower layers of BERT encode more local syntax information such as part-of-speech (POS) tags, while the higher layers tend to capture more complex semantics relying on wider contexts.
  
- It's reasonable to initialized the transformer layers by the last K layers of pre-trained BERT, and are updated during the training.
</script></section><section data-markdown><script type="text/template">
## Classifier component

- DC-BERT treats the document reranking task as a binary classification problem to predict whether the retrieved document is relevant to the question.

- We parameterize the binary classifier as a MLP layer on top of the Transformer layers:
    ![](attachments/2021-10-12-13-38-59.png)
</script></section></section><section ><section data-markdown><script type="text/template">
## Experiments <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Benchmark Datasets

1. SQuAD-Open: Composed of questions from the original crowdsourced SQuAD dataset.

2. Natural Questions Open: Composed of questions from the original Natural Questions dataset.
</script></section><section data-markdown><script type="text/template">
## Evaluation Metrics

- Retriever speedup: Running on a single GPU.

- Retriever ranking performance:

  1. P@N: The percentage of questions for which the answer span appears in one of the top N documents. i.e. $	\frac{r}{n}$ , where n can be chosen based on an assumption about how many documents the user will view.
  </script></section><section data-markdown><script type="text/template">
## Evaluation Metrics

  2. PBT@N: The percentage of questions for which at least one of the top N documents that contains the answer span is not in the TF-IDF top *N* documents.
       - The higher the better

  3. PTB@N: the percentage of questions for which at least one of the top N TF- IDF retrieved documents that contains the answer span not in the retriever’s top *N* documents.
       - The lower the better

</script></section><section data-markdown><script type="text/template">
## Evaluation Metrics

  4. To evaluate the downstream QA performance, the paper uses exact match (EM) score. This  metric  measures  the  percentage of predictions that match any one of the ground truth answers exactly.
</script></section><section data-markdown><script type="text/template">
## Baseline

1. BERT-base: A well-trained reranker using the BERT-base model.

2. Quantized BERT: A compressed BERT-base model.

3. DistilBERT: Leverages knowledge distillation techniques to train a smaller and compact student BERT model.

</script></section><section data-markdown><script type="text/template">
## Results

![](attachments/2021-10-13-17-22-59.png) <!-- .element: class="img100" -->
</script></section><section data-markdown><script type="text/template">
## Results

![](attachments/2021-10-13-17-25-24.png) <!-- .element: class="img100" -->
</script></section><section data-markdown><script type="text/template">
## Ablation Study

- Replace the transformer layer with:
    1. linear layers (DC-BERT-Linear)
    2. LSTM layer (DC-BERT-LSTM)

![](attachments/2021-10-14-11-23-13.png) <!-- .element: class="img80" -->
</script></section><section data-markdown><script type="text/template">
## Ablation Study

- Different number of retrieved documents & model architecture

![](attachments/2021-10-14-11-25-40.png) <!-- .element: class="img80" -->
</script></section><section data-markdown><script type="text/template">
## Ablation Study

- Different number of transformer layers

![](attachments/2021-10-14-12-49-00.png) <!-- .element: class="img80" -->
</script></section></section><section ><section data-markdown><script type="text/template">
## Conclusion <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Conclusion

- This paper introduces DC-BERT to decouple question and document for efficient contextual encoding.

- DC-BERT has been successfully applied to document retrieval, a key component in opendomain QA, achieving 10x speedup while retaining most of the QA performance.
</script></section><section data-markdown><script type="text/template">
## Conclusion

![](attachments/2021-10-14-17-04-11.png) <!-- .element: class="img90" --></script></section></section></div>
    </div>

    <script src="./dist/reveal.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"history":true,"center":true,"slideNumber":"h.v","pdfMaxPagesPerSlide":1,"pdfSeparateFragments":true,"pdfPageHeightOffset":-1,"transition":"none"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
