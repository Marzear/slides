<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>Seminar - 20220610</title>
    <link rel="shortcut icon" href="./favicon.ico"/>
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="_assets/theme/mytheme.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">

# ProphetChat: Enhancing Dialogue Generation with Simulation of Future Conversation <!-- .element: class="title" -->
## Liu et al., ACL 2022<!-- .element: class="subtitle" -->

<div class="title-name">
2022.06.10 <br>
Yu-Hung Wu @ IIS, Academia Sinica
</div>

https://aclanthology.org/2022.acl-long.68/ <!-- .element: class="footnote" -->
</script></section><section  data-markdown><script type="text/template">
## Outline

- Previous Model
- Proposed Model
- Results & Studies
- Conclusion
</script></section><section ><section data-markdown><script type="text/template">
## Previous Model  <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Dialogue Generation in Open-domain Conversations

- Recent dialogue systems usually utilize the dialogue history to generate the response, i.e. estimate the probability of $P(response|history)$.

- There are some common problems in such systems:

    1. Generating bland responses (e.g., yeah, ha ha) is the most critical problem.

    2. It also poses a greater **one-to-many** problem than is typical in other text generation tasks.

    3. The underlying linguistic patterns in human conversations can be highly different from those in general text.
</script></section><section data-markdown><script type="text/template">
## DialoGPT (Zhang et al., 2020)

- DialoGPT is formulated as an autoregressive language model, and uses the multi-layer transformer as model architecture.

- The model is trained on large-scale dialogue pairs/sessions extracted from Reddit discussion chains.

- DialoGPT successfully captures the joint distribution of $P(target|source)$ in conversational flow, which achieves SOTA results in 2020.
</script></section><section data-markdown><script type="text/template">
## DialoGPT (Zhang et al., 2020).

- First, concatenate all dialog turns within a dialogue session into a long text $S = x_{1}, x_{2}, ... , x_{m}$ ($m$ is the sequence length), ended by the ```<end-of-text>``` token.

- The conditional probability of $P(T|S)$ can be written as the product of a series of conditional probabilities:
    ![](attachments/2022-06-08-21-46-12.png)

    where $m$ is the length of the history sequence.
</script></section><section data-markdown><script type="text/template">
## Maximum Mutual Information (MMI)

- MMI is a solution the problem that the response was sometimes too bland or uninformative.

- Given a dialogue in the training set:
  - A: How old are you?
  - B: I'm 22 years old.
  - A: Oh, that's terrible.

- To train the target model, we use the following as input sequence:
  - ```[CLS]How old are you?[SEP]I'm 22 years old.[SEP]Oh, that's terrible.[SEP]```
</script></section><section data-markdown><script type="text/template">
## Maximum Mutual Information (MMI).

- Now, we train an additional "MMI model". MMI is the backward version of the original model, it's goal is to *predict source sentences from given responses.*.
    - Original training objective: maximize $P(Target|Source)$, MMI training objective: $P(Source|Target)$.

- Thus, the input is the **reversed** sentence of the original model:
  - ```[CLS]Oh, that's terrible.[SEP]I'm 22 years old.[SEP]How old are you?[SEP]```
</script></section><section data-markdown><script type="text/template">
## How to Use the MMI Model?

1. Use the target model to generate a set of responses (use different sampling method to generate diverse responses, such as top-k), we denote it as *S*.
    - $S = [r_{1}, r_{2}, ... ,r_{n}]$, where $n$ is the number of candidate responses.

2. for each response in $S$, use the MMI model to calculate it's perplexity score. The input of the MMI model is backwarded.

3. Choose the one with least perplexity, this is the final response.
</script></section></section><section ><section data-markdown><script type="text/template">
## Proposed Model  <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Motivation

- Since there exists a ***one-to-many*** relationship in dialogue generation, generating a desired response solely based on the historical information is not easy.

- If the chatbot can foresee in advance what the user would talk about after receiving its response, it could possibly provide a more informative response.

- Previous models usually consider $P(response|history)$, the proposed model considers $P(response|history, future)$.
</script></section><section data-markdown><script type="text/template">
## Motivation.

- Some PLMs such as BERT use bi-directional information instead of just considering the tokens on its left side, and have bring significant improvement.

- The model thus uses the "right side information" (i.e. dialogue future) to enhance the generated response.
</script></section><section data-markdown><script type="text/template">
## Example

![](attachments/2022-06-08-21-47-26.png) <!-- .element: class="img65" -->
</script></section><section data-markdown><script type="text/template">
## Overall Framework
![](attachments/2022-06-08-21-48-36.png) <!-- .element: class="img110" -->

- Generate responses → select $k$ → generate futures → select $k$ → generate more responses → select 1
</script></section><section data-markdown><script type="text/template">
## Generating First-pass Responses

![](attachments/2022-06-08-21-48-26.png) <!-- .element: class="img80" -->

- Given a dialogue history $h$, first use $G_{F}$ (a forward generator, you can just regard as a normal GPT-2) to generate $n$ responses using top-$k$ sampling.

- The selector $S$ then calculates the quality scores for all history-response pairs.

- Keep the $k$-best responses at hand while discarding the others.
</script></section><section data-markdown><script type="text/template">
## Generating Futures

![](attachments/2022-06-08-21-49-57.png) <!-- .element: class="img80" -->

- Now, we have $k$ first-pass responses. We use $G_{F}$ (**exactly the same as the previous $G_{F}$**) to generate $n$ futures for each response. Thus, we obtain $k · n$ history-response-future dialogue triplets.

- We again use to the selector $S$ to calculate the quality scores of all the generated futures.
</script></section><section data-markdown><script type="text/template">
## Generating Futures.

![](attachments/2022-06-08-21-49-57.png) <!-- .element: class="img80" -->

- Considering that the responses are not equal in quality, we additionally multiply each score of future with the score of its ancestral response to get the final ranking score.

- Again, we keep the $k$-best futures at hand while discarding the others.
</script></section><section data-markdown><script type="text/template">
## Ensemble Generation

![](attachments/2022-06-09-00-38-58.png) <!-- .element: class="img80" -->

- The goal of ensemble generation is to generate to second-pass responses (i.e. the responses that has consider the simulated futures).

- The responses are generate base on both *history-conditioned* $G_{F}$ (yes, the same $G_{F}$ again) and *future-conditioned* $G_{B}$ (MMI model).
</script></section><section data-markdown><script type="text/template">
## Ensemble Generation.

- The response $r$ using the per-step weighted ensemble of $G_{F}$ and $G_{B}$ conditioned on $h$ and $f$:

  ![](attachments/2022-06-09-00-46-17.png) <!-- .element: class="img70" -->

- $w$ is a ***learnable*** weight:
  - Using a ***trainable*** gate $g$ which takes the last hidden states from $G_{F}$ and $G_{B}$ as inputs and calculates an ensemble weighting score w using an MLP with sigmoid activation.

- We use the above method to generate $n$ responses for each future, and thus generate $k · n$ responses.
</script></section><section data-markdown><script type="text/template">
## Final Response Generation

![](attachments/2022-06-09-01-08-13.png) <!-- .element: class="img80" -->

- To make full use of the $k$-best first-pass responses, we finally re-rank the $k + k · n$ responses with $S$ and consider the top-ranked response as our system outputs.
</script></section><section data-markdown><script type="text/template">
## Framework Summary

![](attachments/2022-06-08-21-48-36.png) <!-- .element: class="img70" -->

- There are 4 components in the framework:
  1. $G_{F}$: the forward model.
  2. $G_{B}$: the backward model.
  3. $g$: the gate used to compute the weight between $G_{F}$ and $G_{B}$.
  4. $S$: the selector to evaluate the utterances.

- Generate responses → select $k$ → generate futures → select $k$ → generate more responses → select 1</script></section></section></div>
    </div>

    <script src="./dist/reveal.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"history":true,"center":true,"slideNumber":"h.v","pdfMaxPagesPerSlide":1,"pdfSeparateFragments":true,"pdfPageHeightOffset":-8,"transition":"none"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
