<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>seminar - 20211224</title>
    <link rel="shortcut icon" href="./favicon.ico" />
    <link rel="stylesheet" href="./dist/reset.css" />
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="./_assets/theme/mytheme.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />


  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">

# PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable <!-- .element: class="title" -->
## ACL 2020 <!-- .element: class="subtitle" -->

<div class="title-name">
2021.12.24 <br>
Yu-Hung Wu @ Academia Sinica
</div>

https://arxiv.org/abs/1910.07931 <!-- .element: class="footnote" -->
</script></section><section  data-markdown><script type="text/template">
## Outline

- Introduction
- PLATO
- Experiments
- Conclusion
</script></section><section ><section data-markdown><script type="text/template">
## Introduction  <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Background

- Dialogue generation in open-domain conversations is a challenging task, due to
    1. limited corpus of human conversations.
    2. complex background knowledge.
    3. Diverse relationships between utterances.

- Pre-trained large-scale language models can be used for generation after being fine-tuned.

- However, there are some deficiencies in performance while directly fine-tuning BERT on small conversation datasets.
</script></section><section data-markdown><script type="text/template">
## Problems

1. The underlying linguistic patterns in human conversations can be highly different from those in general text.

2. The training mode of uni-directional dialogue generation is also distinct from that of bi-directional natural language understating as applied in BERT.

3. There exists a ***one-to-many*** relationship in dialogue generation.
</script></section><section data-markdown><script type="text/template">
## Solutions

1. The underlying linguistic patterns in human conversations can be highly different from those in general text.
    - Large-scale Reddit and Twitter conversations are utilized.
2. The training mode of uni-directional dialogue generation is also distinct from that of bi-directional natural language understating as applied in BERT.
    - A flexible paradigm integrating uni- and bi-directional processing is employed.
3. There exists a ***one-to-many*** relationship in dialogue generation.
    - Discrete latent variable is introduced.
</script></section><section data-markdown><script type="text/template">
## Latent Variable

- The latent variable gets exempted from the restriction of human annotations and can be learned automatically from the corpus.

- There are 2 tasks in this work:
    1. Response generation: Based on the context and latent variable, the generation task tries to maximize the likelihood of the target response.

    2. Latent act recognition: Based on the given context and target response, the task aims to estimate the latent variable.
</script></section></section><section ><section data-markdown><script type="text/template">
## PLATO <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Basic Idea

- The main purpose is to encode discrete latent variables into transformer blocks for one-to-many relationship modeling.

- In the model, there are 3 elements:
    1. dialogue context (dialogue history, knowledge...) $c$
    2. target response $r$
    3. latent variable $z$: a $K$-way categorical variable $z ∈ [1, K]$
</script></section><section data-markdown><script type="text/template">
## Probabilistic Relationships

- Response generation (gray lines) - Given a context $c$ and latent variable $z$, conditioned on the context and one selected latent speech act, the response $r$ is generated as $p(r|c, z)$.

- Latent act recognition (dashed blue lines): Given a pair of context $c$ and response $r$, the underlying latent speech act $z$ can be estimated as $p(z|c, r)$.

![](attachments/2021-12-22-17-54-36.png) <!-- .element: class="img95" -->
</script></section><section data-markdown><script type="text/template">
## Model Architecture - 1 

![](attachments/2021-12-22-18-00-58.png) <!-- .element: class="img100" -->

- The latent act recognition shares network parameters with response generation.
- The response generation is a uni-directional decoding process, each token in the response only attends to those before it.
</script></section><section data-markdown><script type="text/template">
## Model Architecture - 2 

![](attachments/2021-12-22-18-00-58.png) <!-- .element: class="img100" -->

- The latent act recognition works with a special mask symbol $[M]$ as input, it keeps collecting information from the context and target response.
</script></section><section data-markdown><script type="text/template">
## Input Representation

![](attachments/2021-12-22-18-14-34.png) <!-- .element: class="img100" -->

- Given that $z$ is one $K$-way categorical variable, its token embedding $E[z]$ is mapped from the latent embedding space $ E_{z} ∈ \mathbb{R}^{K×D} $.

- Role embeddings are employed to differentiate the characters evolved in the conversation.
</script></section><section data-markdown><script type="text/template">
## Latent Embeddings
```python
'''
post_embed.size = [batch_sz, BERT dimension, seq length]
post_logits.size = [batch_sz, K]
latent_embeddings.size = [batch_sz, K, BERT dimension]
'''
post_embed = bert_embed[:, 0]
post_logits = self.post_network(post_embed) #FC
if is_training:
    z = F.gumbel_softmax(post_logits, self.tau)
else:
    indices = layers.argmax(post_logits, axis=1)
    z = layers.one_hot(F.unsqueeze(indices, [1]), self.num_latent)
latent_embed = layers.matmul(z, latent_embeddings)
```

* ```post_embed``` is the final hidden state of token $[M]$ (in the latent act recognition task).
* ```post_network``` is a fully connected layer, mapping the embedding from BERT dimension to $K$.
* ```latent_embeddings``` is a $[K, 768]$ matrix, mapping $z$ to the latent variable token embedding $E[z]$.

</script></section><section data-markdown><script type="text/template">
## Pre-training Objectives

- In this task, there are 3 losses:
    1. negative log-likelihood (NLL) 
    2. bag-of-words (BOW)
    3. response selection (RS) loss

- The total objective of our pre-training model is to minimize the integrated loss:
    ![](attachments/2021-12-22-18-28-15.png)
</script></section><section data-markdown><script type="text/template">
## Negative Log-likelihood (NLL) loss 

![](attachments/2021-12-22-18-30-11.png) <!-- .element: class="img60" -->

- $z$ is the latent speech act of this training pair $(c, r)$, sampled from the probability distribution $p(z|c, r)$.

- $p(z|c,r)$ is estimated through the task of latent act recognition:
    ![](attachments/2021-12-22-18-32-48.png)
    where $h[M] ∈ \mathbb{R}^{D}$ is the final hidden state of the special mask, $W_{1} ∈ \mathbb{R}^{K×D}$ and $b_{1} ∈ \mathbb{R}^{K}$ denote
the weight matrices of one fully-connected layer.
</script></section><section data-markdown><script type="text/template">
## BOW (bag-of-words) loss

![](attachments/2021-12-23-17-09-12.png)

- Aims to facilitate the training process of latent discrete variables.

- The function $f$ tries to predict the words within the target response in a non-autoregressive way:

![](attachments/2021-12-23-17-11-31.png)

where $h_{z}$ is the final hidden state of the latent variable and $|V|$ is the vocabulary size. $f_{r_{t}}$ denotes the estimated probability of word $r_{t}$.
</script></section><section data-markdown><script type="text/template">
## NLL loss vs. BOW loss

- NLL loss is one of a useful metrics in response generation.

- BOW loss discards the order of words and forces the latent variable to capture the global information of the target response.
</script></section><section data-markdown><script type="text/template">
## RS (response generation) loss - 1

- Response selection helps distinguish whether the response is relevant with the dialogue context and consistent with the background knowledge.

-  its score can be regarded as an indicator of coherence during inference, helping to select the most coherent one from multiple candidate responses.
</script></section><section data-markdown><script type="text/template">
## RS (response generation) loss - 2

- The positive training samples come from the dialogue context and corresponding target response $(c, r)$, with label $l_{r} = 1$.

- The negative samples are created by randomly selecting responses from the corpus $(c, r^{−})$, with label $l_{r^{−}} = 0$.

- The binary cross-entropy loss of response selection is defined as follows:![](attachments/2021-12-23-17-31-35.png)
</script></section><section data-markdown><script type="text/template">
## Pre-training Summary

1. Latent Act Recognition

    - Given a pair of context and target response, estimate the posterior distribution $p(z|c, r)$.
    - Randomly select $r^{-}$ and calculate $L_{RS}$.

2. Response Generation
    - With the sampled latent value $z ∼ p(z|c, r)$, calculate $L_{NLL}$ and $L_{BOW}$.

3. Optimization
    - Sum up to obtain $L$, and update network parameters with back-propagation.
</script></section><section data-markdown><script type="text/template">
## Hyper-parameters Used in Pre-training

- Max length of context = 256, response = 50

- Transformer blocks = 12, dimension = 768

- batch size = 64, K (number of latents) = 20

- 8 Nvidia Tesla V100 32G GPU cards for 3.5M steps, taking about two weeks to reach convergence.
</script></section><section data-markdown><script type="text/template">
## Fine-tuning

- The fine-tuning on small conversation datasets can be carried out by following the training objectives of the pre-training procedure.

- During inference, we can
    1. Conditioned on each latent value $z ∈ [1, K]$, generate corresponding candidate response $r$.
    2. Calculate the probability for each response $p(l_{r} = 1|c, r)$ and select the one with highest coherence value as the final response.
</script></section></section><section ><section data-markdown><script type="text/template">
## Experiments <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Datasets

- Persona-Chat: a knowledge grounded conversation dataset.

- Daily Dialog: a chit-chat dataset, which contains high-quality human conversations about daily life.

- DSTC7-AVSD: a conversational question answering dataset.
    -  the system need to generate an answer given dialogue context and background knowledge.

![](attachments/2021-12-23-18-04-29.png) <!-- .element: class="img100" -->
</script></section><section data-markdown><script type="text/template">
## Baseline

- Persona-Chat and Daily Dialog: Sequence to sequence with attention

- DSTC7-AVSD: DSTC7 baseline system
</script></section><section data-markdown><script type="text/template">
## Evaluation Metrics

- BLEU score: measures the n-gram overlap between generated response and the target response.

- Distinct-1/2: measures the generation diversity, which is defined as the number of distinct uni- or bi-grams divided by the total amount of generated words

- Knowledge Recall/Precision/F1: measures the degree of informativeness.
</script></section><section data-markdown><script type="text/template">
## Experiments - 1

![](attachments/2021-12-23-18-07-48.png) <!-- .element: class="img100" -->
</script></section><section data-markdown><script type="text/template">
## Experiments - 2

![](attachments/2021-12-23-18-14-37.png) <!-- .element: class="img100" -->
</script></section><section data-markdown><script type="text/template">
## Sample responses

![](attachments/2021-12-23-18-32-25.png) <!-- .element: class="img100" -->
</script></section><section data-markdown><script type="text/template">
## Ablation Studies

![](attachments/2021-12-23-18-41-16.png) <!-- .element: class="img100" -->

* Model 1.2 vs. 1.3: Reflects that bi-directional attention mask should apply to context information.

* Group 2 vs. 1: Reflects Twitter and Reddit conversations do improve the generated text.

* Model 2.2 vs. 3.1: Reflects that discrete latent variable is able to boost the quality of response generation.
</script></section></section><section ><section data-markdown><script type="text/template">
## Conclusion <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Conclusion

- Incorporated with latent discrete variables, PLATO has the ability to deal with one-to-many relationship modeling.

- Two reciprocal tasks of response generation and latent recognition are carried out simultaneously on large-scale conversation datasets.

- The results demonstrate that our model obtains significant improvements over the other state-of-the-art methods.

- The model can be potentially improved with more fine-grained latent variables.</script></section></section></div>
    </div>

    <script src="./dist/reveal.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"history":true,"center":true,"slideNumber":"h.v","pdfMaxPagesPerSlide":1,"pdfSeparateFragments":true,"pdfPageHeightOffset":-1,"transition":"none"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
