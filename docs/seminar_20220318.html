<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>Seminar - 20220318</title>
    <link rel="shortcut icon" href="./favicon.ico" />
    <link rel="stylesheet" href="./dist/reset.css" />
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="./theme/mytheme.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />


  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">

# E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning (ACL 2021)<!-- .element: class="title" -->
## Alibaba Group<!-- .element: class="subtitle" -->

<div class="title-name">
2022.03.18 <br>
Yu-Hung Wu @ Academia Sinica
</div>

https://arxiv.org/abs/2106.01804 <!-- .element: class="footnote" -->
</script></section><section  data-markdown><script type="text/template">
## Outline

- Previous Models
- Proposed Model
- Results & Studys
- Conclusion
</script></section><section ><section data-markdown><script type="text/template">
## Previous Models  <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Vision-Language Pre-training (VLP)

- In the past few years, the emergence of pre-training models has brought uni-modal fields such as CV and NLP to a new era.

- Vision-Language Pre-training (VLP) mainly learns the semantic correspondence between different modalities (image-text, video-text, etc.) by pre-training on large-scale data.

- We then fine-tunes VLP models on task-specific data and achieving state-of-the-art results on various downstream V+L tasks.
</script></section><section data-markdown><script type="text/template">
## Previous Models

- Most existing mainstream VLP models adopt a **two-step training** method:
    1. Extracts semantic visual features using a pre-trained object detection model (FRCNN, Yolov4...).
    2. combines the derived object-centric representation of the image and text embedding as the input of Transformer.
</script></section><section data-markdown><script type="text/template">
## VL-BERT (Su, 2020)

- Two-stage training: FRCNN → BERT

![](attachments/2022-03-16-11-30-49.png) <!-- .element: class="img100" -->

https://arxiv.org/abs/1908.08530 <!-- .element: class="footnote" -->
</script></section><section data-markdown><script type="text/template">
## The Problems of Two-stage Training

- The object detection model in the first step is trained on specific visual dataset, visual representation is not optimized towards a more generic cross-modal understanding in the second step.
    - When the object detection model fails to recognize certain important information, It may suffer from an error propagation problem.

-  Extracting region features with an object detection model is so time-consuming.
</script></section><section data-markdown><script type="text/template">
## The Problems of Two-stage Training.

- Several studies have begun to revisit the grid features for cross-modal understanding.

- **Pixel-BERT** explores to pre-train with grid features in an end-to-end fashion directly from pixels. It removes all the fine-grained visual pre-training tasks.

- However, **Vinvl** demonstrates that visual features provided by the object detection model matter significantly in VLP models.
</script></section><section data-markdown><script type="text/template">
## Pixel-BERT (Huang, 2020)

![](attachments/2022-03-16-13-51-00.png) <!-- .element: class="img110" -->

https://arxiv.org/abs/2004.00849 <!-- .element: class="footnote" -->
</script></section></section><section ><section data-markdown><script type="text/template">
## Proposed Model <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## E2E-VLP

- A Transformer encoder-decoder framework 

![](attachments/2022-03-16-13-56-48.png) <!-- .element: class="img90" -->
</script></section><section data-markdown><script type="text/template">
## Encoder
<div id="left">

- The text embeddings are similar to BERT.

![](attachments/2022-03-16-16-30-07.png) <!-- .element: class="img70" -->

- For image embeddings, it uses a simple CNN backbone module. It is used as the image encoder for extracting visual representations from pixels.

</div>

<div id="right">

![](attachments/2022-03-16-14-12-03.png) <!-- .element: style="float: right; width: 50%" --> <!-- .element: class="img110" -->

</div>
</script></section><section data-markdown><script type="text/template">
## Encoder.
<div id="left">

- Starting from the initial image: $v_{img}\in R^{3\times H_{0}\times W_{0}}$

- A conventional CNN backbone generates a lower resolution activation map: $f_{img} = CNN(v_{img})\in R^{C\times H\times W}$, where $C=2048$ and $H=\frac{H_0}{32}, W=\frac{w_0}{32}$

- Use a $1\times1$ convolution to reduce the channel dimension from $C$ to $d$, creating a new feature map $z_{img}\in R^{d\times H\times W}$.

</div>

<div id="right">

![](attachments/2022-03-16-14-12-03.png) <!-- .element: style="float: right; width: 50%" --> <!-- .element: class="img110" -->

</div>
</script></section><section data-markdown><script type="text/template">
## Encoder..
<div id="left">

- In order to input the image into the Transformer, we collapse the spatial dimensions of $z_{img}$ into one dimension, resulting in a $HW\times d$ feature map $Z_{img}$. 

- Supplement the feature maps with fixed positional encodings.

- Finally, the resulting sequential image representation is $Z_{img}=\{o_1,...,o_{HW}\}$.

</div>

<div id="right">

![](attachments/2022-03-16-14-12-03.png) <!-- .element: style="float: right; width: 50%" --> <!-- .element: class="img110" -->

</div>
</script></section><section data-markdown><script type="text/template">
## Encoder...
<div id="left">

- we directly concatenate the derived image features and text embeddings to construct the input sequence, which is formulated as: $\{e_{CLS},e_{1},...,e_{m},e_{SEP},o_1,...,o_{HW}\}$.

</div>

<div id="right">

![](attachments/2022-03-16-14-12-03.png) <!-- .element: style="float: right; width: 50%" --> <!-- .element: class="img110" -->

</div>
</script></section><section data-markdown><script type="text/template">
## The Pretraining of Encoder

- Masked Language Modeling (MLM)

  - The task setup is basically the same as in BERT.

  - Different from BERT, the masked words will be predicted with the help of image feature map from visual modality so as to resolve ambiguity.
</script></section><section data-markdown><script type="text/template">
## The Pretraining of Encoder.

- Image-Text Matching

    - randomly sample 50\% mismatched image-text pairs and 50\% matched pairs, and train an classifier to predict whether an image and a sentence match each other on the representation of token [CLS] in the last encoder layer $h^L_{CLS}$.
</script></section><section data-markdown><script type="text/template">
## "Visual-enhanced" Decoder

<div id="left">

- CNN feature map has no object level semantics. Therefore, we add a Transformer decoder to help capture the fine-grained semantics of the visual features.

- Two specific pre-training tasks are incorporated:

    - Object detection
    - Image-caption generation

</div>

<div id="right">

![](attachments/2022-03-16-18-26-02.png) <!-- .element: style="float: right; width: 50%" --> <!-- .element: class="img110" -->

</div>
</script></section><section data-markdown><script type="text/template">
## Enhanced by Object Detection

- Largely based on DETR

    - ＋cross-modal input

    - ＋object attribute prediction
  
- These input embeddings are learnt positional encodings that we refer to as object queries

![](attachments/2022-03-16-18-40-48.png) <!-- .element: class="img110" -->

https://arxiv.org/abs/2005.12872 <!-- .element: class="footnote" -->
</script></section><section data-markdown><script type="text/template">
## Enhanced by Object Detection.

- Every prediction is computerd by a 3-layer perceptron and a linear layer:
  - The MLP predicts the normalized center coordinates, height and width.

  - The linear layer predicts the class label.

  - A sepcial label "no object" $\varnothing$: used to represent that no object is detected within a slot.
</script></section><section data-markdown><script type="text/template">
## Enhanced by Object Detection..

- Set-based loss of bipartite matching

  - $y$ = the ground truth set of objects.

  - The output of Transformer decoder: ![](attachments/2022-03-17-13-40-00.png) <!-- .element: class="img30" -->

  - Search for a permutation of $N$ elements $\sigma \in \mathscr{L}_N$ with the lowest cost: ![](attachments/2022-03-17-13-44-49.png) <!-- .element: class="img80" -->

  - Use the Hungarian algorithm to find the optimal assignment.
</script></section><section data-markdown><script type="text/template">
## Enhanced by Object Detection...

- The model is trained with a negative log-likelihood loss for attribute, class prediction and a box regression loss defined as follows:

![](attachments/2022-03-17-15-15-35.png)  <!-- .element: class="img80" -->
</script></section><section data-markdown><script type="text/template">
## Enhanced by Image Captioning

- A sequence-tosequence (Seq2Seq) image-to-text generation task

![](attachments/2022-03-17-15-20-48.png) <!-- .element: class="img80" -->

- $\mathcal{X}$ represents the sequence of **vision** context 
- $\mathcal{Y}$ represents the set of text to be generated
</script></section><section data-markdown><script type="text/template">
## Joint Training

- Minimizing the four loss functions:
![](attachments/2022-03-17-15-38-26.png) <!-- .element: class="img100" -->
 
$\mathcal{L}_{mlm}$ = Masked Language Modeling loss

$\mathcal{L}_{itm}$ = Image-Text Matching loss

$\mathcal{L}_{v}$ = Object Detection loss

$\mathcal{L}_{dec}$ = Image-to-Text Generation (decoder) loss
</script></section></section><section ><section data-markdown><script type="text/template">
## Results & Studys <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Downstream Tasks

- **VQA v2.0**: The VQA task requires the model to answer natural language questions given an image.

    - picking an answer from a shared set consisting of 3,129 answers.

- **NLVR2**: Determine whether a natural language statement is true about a *pair* of images.

![](attachments/2022-03-17-16-03-39.png) <!-- .element: class="img70" -->
</script></section><section data-markdown><script type="text/template">
## Downstream Tasks

- **Image Caption**: A visual generation task that requires the model to generate the content of an image.

- **Image-Text Retrieval**: The image-text retrieval task consists of two sub-tasks: image retrieval and text retrieval, depending on which modality is used as the retrieval target.
</script></section><section data-markdown><script type="text/template">
## Results

![](attachments/2022-03-17-16-22-33.png) <!-- .element: class="img90" -->

- VLP has the potential of removing the complex procedure of region feature extraction.
</script></section><section data-markdown><script type="text/template">
## Results

![](attachments/2022-03-17-17-10-26.png) <!-- .element: class="img100" -->
</script></section><section data-markdown><script type="text/template">
## Ablation

![](attachments/2022-03-17-17-27-43.png) <!-- .element: class="img70" -->

- Object Detection is more important then Image-to-Text Generation: 
  - Downstream tasks such as VQA and NLVR2 focus more on the fine-grained semantics of the objects within image.
</script></section><section data-markdown><script type="text/template">
## Inference Efficiency

![](attachments/2022-03-17-17-38-54.png) <!-- .element: class="img70" -->

- For two-step VLP models (LXMERT, UNITER), about **80%** of the total time is used to extract region-based features using FRCNN.
</script></section><section data-markdown><script type="text/template">
## Architecture Selection

![](attachments/2022-03-17-17-56-34.png) <!-- .element: class="img70" -->

- The more the layers, the better the performance.

- More strong visual backbone > more Transformer layers: visual semantic understanding is rather important → fine-grained
visual pre-training tasks.
</script></section><section data-markdown><script type="text/template">
## The Impact of Image Size

![](attachments/2022-03-17-18-13-12.png) <!-- .element: class="img70" -->
</script></section><section data-markdown><script type="text/template">
## Object Detection with Paired Text

![](attachments/2022-03-17-18-21-24.png) <!-- .element: class="img70" -->

- Encode both the image content and caption text with E2E-VLP, and directly fine-tune it on MSCOCO object detection benchmark dataset with the decoder as in DETR.
</script></section></section><section ><section data-markdown><script type="text/template">
## Conclusion <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Conclusion

- New end-to-end paradigm for pixel-level vision-language pre training.

- A Transformer encoder-decoder architecture:
  - Encoder: cross-modal input, grid-based image input
  - Decoder: Object detector + Image-to-text generation</script></section></section></div>
    </div>

    <script src="./dist/reveal.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"history":true,"center":true,"slideNumber":"h.v","pdfMaxPagesPerSlide":1,"pdfSeparateFragments":true,"pdfPageHeightOffset":-1,"transition":"none"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
